--- 
layout: post
title: 文本挖掘基础：文本特征选择
tags: 
- "机器学习"
- "自然语言处理"
- "特征选择"
status: publish
type: post
published: true
---
当使用向量空间模型来表示文本时，每一个不同的单词都作为特征空间中的一维，每一个文本是特征空间中的一个向量。这种描述方法使得文本向量空间变得非常的高维而且稀疏，从而可能导致性能低下并误导挖掘算法。因此，需要对文本特征项进行进一步地选择，选取一部分最具代表性的特征,以降低特征空间的维数。

特征项的选择原则

特征项通常用文本所包含的基本语义单位(字，词，词组或短语等)来表示，选出的特征项越具有代表性，语言层次越高，所包含的信息就越丰富，特征项的选取应遵循以下三个原则：


1) 选取那些包含语义信息较多的，对文本的表示能力较强的语言单位作为特征项；

2) 选取的过程本身应当比较容易实现，其时间和空间开销都不应过大；

3) 文本在这些特征项上的分布应当有比较明显的统计规律性；

对于中文来说，常见的特征性主要包含“字特征”和“词特征”。字对文本的表示能力相对于词比较差，经常不能完整、独立地表达语义信息；而词特征对文本的表示能力比较好，词汇能够比较完整地表达语义信息。

大部分情况下，选择词作为特征项进行分析比较合理，然而并不是所有词都适合作为特征项，研究表明，高频词和低频词对文本主题意义的表达均小于中频词。因为高频词在所有文章中都有相近的较高频率；低频词在文本中出现次数少，不适合采用统计方法来处理；而中频词和文本表达的主题比较相关，表示能力最强，可以用来区分不同内容的文本。

对于文本中常常出现一些没有实在意义的虚词、助词等等，比如“的”、“得”，这些词出现次数很多，对于文本分析基本没有意义。常用的方法是建立停用词表，过滤掉这些词。

特征项的选择方法

一般通过一些衡量特征项重要性的评价函数来进行文本特征选择，首先根据这个评价函数计算出每一个特征项的重要性评分，然后对所有的特征项按评分大小进行排序，最后依据从大到小的顺序选取一定数目的特征项作为文本的特征子集。

有监督的特征选择算法通过计算类与特征项之间的关系来选择最有区分力的特征子集，主要有信息增益(Information Gain)、期望交叉熵(Expected Cross Entropy)、卡方统计(Chi-Square)和互信息(Mutual Information)等；无监督的特征选择算法主要有文档频率(Document Frequency)、特征增强(Term Strength)等；

(1) 信息增益(Information Gain)

信息增益是一种基于熵的评估方法，对于一个特征项来说，在不知道该特征项的条件下，所有类有一个平均无条件的信息熵H(C),其中C为类别变量，它可能的取值包括C1、C2、…、Cn；在知道这个特征项之后，所有类就会有一个平均条件(条件就是知道该特征词)的信息熵H(C|t),这就是所谓的条件熵，其中t为特征项。两个信息熵的差值H(C)-H(C|t)就是信息增益，平均无条件信息熵H(C)及条件熵H(C|t)的定义如下：

H(C) = -P(C1)*logP(C1) -P(C2)*logP(C2) … -P(Cn)*logP(Cn)

H(C|T) = P(t)*H(C|t) + P(!t)*H(C|!t)

特征项的信息增益值越大,表明其贡献越大，对分类越重要,因此在进行特征选择时,通常选取信息增益值大的若干个特征词构造文本的特征向量。

信息增益的主要问题在于它只能考察特征项对整个系统的贡献（所有类别），而不能具体到某个类别上，这就使得它只适合用来做“全局”特征选择（指所有的类都使用相同的特征集合），而无法做“局部”特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。

（2）期望交叉熵(Expected Cross Entropy)

期望交叉熵与信息增益很相似,不同之处在于,期望交叉熵只考虑特征项在文本中发生的情况,而信息增益同时考虑了特征项在文本中发生与不发生两种情况，对于特征项t，其期望交叉熵ECE(t)的定义如下：

ECE(t)=P(t)*{P(C1|t)*log[P(C1|t)/P(C1)]} + P(t)*{P(C2|t)*log[P(C2|t)/P(C2)]} + … + P(t)*{P(Cn|t)*log[P(Cn|t)/P(Cn)]}

其中，P(t)表示特征项t出现的概率，P(Ci)表示Ci类文本出现的概率，P(C1|t)表示文本出现t的条件下类别Ci出现的概率。它反映了文本类别的概率分布和在出现了某个特征项t的条件下文本类别的概率分布之间的距离，特征项t的期望交叉熵越大，对文本类别分别的影响也越大。

（3）卡方统计(Chi-Square)

在统计学中，卡方统计常用来检验两个变量的独立性，在文本分类中我们可以用卡方检验表示特征项t和类别C之间独立性，卡方统计量的值越大，说明实际值和期望值的偏离越大，越倾向于认为卡方检验的原假设（两个变量独立）的反面是正确的，即独立性越小，相关性越大。

比如说现在有N篇文档，其中有M篇是关于体育的，我们想考察一个词“篮球”与类别“体育”之间的相关性。我们有以下四个观察值可以使用（包含“篮球”且属于“体育”类别的文档数为A，等等） 

  
属于“体育类”
 
不属于“体育类”
 
合计
 

包含“篮球”
 
A
 
B
 
A + B
 

不包含“篮球”
 
C
 
D
 
C + D
 

合计
 
M = A + C
 
B + D
 
N = A + B + C + D
 

根据卡方检验，如果原假设是成立的，即“篮球”和“体育类”文章没什么相关性，那么在所有的文章中，“篮球”这个词都应该以等概率(A+B)/N 出现。属于体育类的文章数为A+C，在属于体育类的文档中，理论上应该有E11=(A+C)*(A+B)/N篇包含“篮球”这个词。但实际上属于体育类的文章中有A篇文档出现了“篮球”这个词，因此包含“篮球”且属于“体育类”文章观察值与期望值的相对偏差为D11=(A-E11)/E11,同样的道理，其他三种情况也可以得到D12、D21、D22。有了所有观察值的相对偏差，那么“篮球”特征项和“体育类”类别的卡方值即为D=D11+D12+D21+D22,代入并化简后可得到更一般的形式：

D(t,C) = N*(AD-BC)^2/[(A+C)*(A+B)*(B+D)*(C+D)]

按照以上方式，同样可以计算其他特征项和类别的卡方值，计算完成后，对所有的类别和特征项对(t,C)按照降序排列，去除掉排在后面的特征项。

当然，根据上面的计算过程也可以看到，卡方检验仅仅考虑某一个特征项是否出现，而没有考虑特征项在文档中出现的频率，造成偏袒低频次特征项的缺陷。极端情况下，一个特征项在某一类文章中都只出现一次，其卡方值却大过了在该类文章99%的文档中出现了10次的特征项，其实后面的特征项应该更具代表性，但只因为它出现的文档数比前者少1%,特征选择的时候就有可能保留前者而删掉后者。

（4）互信息(Mutual Information)

互信息根据特征项t与类别C共同出现的概率来度量两者的相关程度。对于特征项t和某一类别C1、C2、…、Cn，在Ci中出现的概率高，而在其他类别中出现概率低的特征项t将获得较高的互信息，两者的相关性较强，也就有可能被选取为类别Ci的特征项。

互信息的定义为：MI(t,Ci) = log{P(t,Ci)/[P(t)*P(Ci)]} = log[P(t|Ci)/P(t)],其中P(t,Ci)表示既包含特征项t又属于类别Ci的文档出现的概率，P(t)表示包含特征项t的文档出现的概率，P(Ci)表示属于类别Ci的文档出现的概率。

根据定义可知，当P(t|Ci)大于P(t)时，MI(t,Ci)大于0，两者正相关，即特征项t的出现说明文档有可能属于类别Ci，反之，说明两者是负相关。而在n个类别集合上特征项t的互信息定义为：MI(t)=MI(t,C1)+MI(t,C2)…+MI(t,Cn)

互信息最大的不足就是对临界特征的概率比较敏感。根据MI(t,Ci)的定义，当两个特征项的条件概率P(t|Ci)值相等时，P(t)较小的词（也即稀有词）比P(t)大的词（也即普通词）的互信息分值要高，因此，对于概率相差太大的两特征项来说，互信息没有考虑特征项在文本中发生的频度，经常倾向于选择稀有特征。如果仅仅使用互信息进行特征项选择，它的精度极低，其主要原因在于它滤掉了很多高频的有用特征。

（5）文档频率(Document Frequency):

DF是最简单的一种特征评估函数，它指的是有多少个文本包含这个单词。该方法是基于这样的一个假设：DF较小的特征项对分类/聚类结果影响较小，而DF较大的特征项对分类/聚类结果影响较大。

该方法优先选取DF较大的特征项而剔除DF较小的特征项，该算法实现简单、计算复杂度低，它的时间复杂度与文本数成线性关系，为O(n)，所以非常适合于超大规模文本数据集的特征选择。另外，DF不需要依赖类信息，是一种无监督的特征选择，在文本预处理过程中，常被用来删除出现次数过少或者出现次数过多的单词以提高后续处理的效率。

(6)特征增强(Term Strength)

该方法最初用于在文本检索中删除一些没有实际意义的特征词，后来又被应用到文本聚类中，它基于这样一个假设：一个特征词如果在相关的文档中出现的越多，在不相关的文档中出现得越少则越重要。

增强的定义为TS(T)=P(t∈di|t∈dj),其中di,dj为文档集D中的文档并且similarity(di,dj)> β, β为用来判断两个文档是否相关的阀值，它计算的是一个词在一对相关文档中的某一个文档中出现的条件下在另一个文档中出现的概率，即条件概率。
