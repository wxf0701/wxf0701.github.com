--- 
layout: post
title: 采用集成学习算法提高分类器的准确性
tags: 
- "机器学习"
- "集成学习"
- "分类"
status: publish
type: post
published: true
---
传统的分类方法是在一个由各种可能的函数构成的空间中寻找一个最接近实际分类函数的分类器。这些单个的分类器模型主要有决策树、人工神经网络、朴素贝叶斯分类器等等。

可以通过聚集多个分类器的预测结果提高分类器的分类准确率，这一方法称为[集成（Ensemble）](http://en.wikipedia.org/wiki/Ensemble_learning)学习或分类器组合（Classifier Combination），该方法由训练数据构建一组基分类器（Base Classifier），然后通过对每个基分类器的预测进行投票来进行分类。

本文首先介绍集成学习算法的基本思想、前提条件，进而讨论集成学习涉及的问题：基本分类器之间的关系、生成基本分类器的方法、基本分类器分类结果的整合，最后对常用的集成学习进行了简单比较。

# 集成学习的基本思想

集成学习的思路是在对新的实例进行分类的时候，把若干个单个分类器集成起来，通过对多个分类器的分类结果进行某种组合来决定最终的分类，以取得比单个分类器更好的性能。如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。

下图表示了集成学习的基本思想，图中的集成分类器包括了t个单一的分类器，对于同样的输入，t分类器分别给出各自的输出，然后这些输出通过整合以后得到集成分类器整体的输出结果作为最终分类。

![集成分类器](/upload/pic/2011-08-24-ensemble-model.png "")

# 集成学习有效的前提条件

虽然集成学习可能取得更好的学习效果，但是并不是所有的集成方式都有效的，集成学习有效需要满足以下两个前提条件：


每个基分类器的错误率都应当低于0.5，否则集成的结果反而会提高错误率。

每个基分类器应该尽可能相互独立，这是因为如果每个基分类器分类结果差不多，则集成后的分类器整体和单个分类器做出的决策实际上没有什么差异。

# 基本分类器之间的关系

按照基分类器之间的种类关系可以把集成学习方法划分为异态集成学习和同态集成学习两种：


+ 异态集成学习：使用各种不同的分类器进行集成，异态集成学习的两个主要代表是叠加法（Stack Generalization）和元学习法（Meta Learning）。

+ 同态集成学习：集成的基本分类器都是同一种分类器，只是这些基本分类器之间的参数有所不同。同态集成的分类器包括有朴素贝叶斯集成、决策树集成、人工神经网络集成、K-近邻集成等。

# 生成多个不同基分类器的方法

1. 处理训练数据集的方法：Bagging

袋装（Bagging）又称自助聚集（Bootstrap AGGregatING）是Breiman在1996年提出的，它是一种根据均匀概率分布从数据集中重复抽样（有放回）的技术，它为每一个基分类器都构造出一个跟训练集同样大小但各不相同的训练集，从而训练出不同的基本分类器。

由于抽样过程是有放回的，因此一个样本可能在同一个训练数据集中出现多次，而其他一些却可能忽略。理论上，自助样本Di大约包含63.2%的原训练数据，因为每一个样本抽到Di的概率为1 – (1-1/N)^N，如果N充分大，这个概率将收敛于1 – 1/e，即大约0.632.

由于Bagging算法本身的特点，使得Bagging算法非常适合用来并行训练多个基本分类器，这也是Bagging算法的一大优势。同时，由于每个样本被选中的概率都相同，Bagging并不侧重于训练数据集中的任何特定样本，所以对于噪声数据，Bagging不太受过分拟合的影响。

2. 处理训练数据集方法：Boosting

Boosting最先由Schapire于1990年提出，其思想是通过一个迭代的过程来自适应训练样本的分布，使得基分类器聚焦在那些很难分类正确的样本上。这个过程就好像一个人背英语单词一样，首先第一遍背完以后有一些容易记住的单词就记住了，还有一些不容易记住的，则第二遍的时候对那些不容易记住的单词多看几眼，第三遍又对第二遍还记不住的单词再多看几眼……

与Bagging不同，具体实施的时候Boosting方法是这样进行的：首先给每一个训练样例赋予相同的权重，然后训练第一个基本分类器并用它来对训练集进行测试，对于那些分类错误的测试样例提高其权重，然后用调整后的带权训练集训练第二个基本分类器，然后重复这个过程直到最后得到一个足够好的学习器。最常用的Boosting算法，是Freund和Schapire于1997年提出的AdaBoost（Adaptive Boosting）算法。

与Bagging对于噪声容忍程度比较好不同，Boosting倾向于那些被错误分类的样本，很容易受过分拟合的影响。与Bagging对训练集的选择是随机的、各轮训练集之间相互独立不同，Boosting对训练集的选择不是独立的，各轮训练集的选择与前面各轮的学习结果有关，各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法，Boosting训练模型将需要大量的时间开销。

3. 处理训练数据集的方法：交叉验证

交叉验证法来自于K-折交叉验证（K-fold Cross Validation）的测试方法，属于无放回抽抽样。K-折交叉验证的思想是把整个训练集划分为互不相交的K等份，然后每次选定一份作为测试集，其他的K-1份作为训练集来进行学习，用这种思路可以训练出K个不同的基本分类器。跟Bagging一样，这种集成方法也适用于多个基本分类器并行进行训练。

4. 处理输入特征的方法

该方法通过选择输入特征的子集来形成每个训练集。子集可以随机选择，也可以根据领域专家的建议选择。相关的研究表明，对那些含有大量冗余特征的数据集，该方法的性能非常好。[随机森林（Random Forest）](http://en.wikipedia.org/wiki/Random_forest)就是采用处理输入特征的集成学习方法，它用决策树作为基分类器。

5. 处理类标号的方法

该方法是用于类别数目足够多的情况，通过将类标号随机划分为两个不相交的子集A0和A1，类标号属于子集A0的训练样本指派到类0，而那些类标号为A1的训练样本被指派到类1。然后，使用重新标记过的数据训练得到一个基分类器，重复重新标记和构建基分类器多次，就得到一组基分类器。当遇到一个检验样本时，使用每个基分类器Ci预测它的类标号。如果检验样本被预测为0/1，则所有属于A0/A1的类都得到一票。最后统计选票，将检验样本的指派到得票最高的类。

6. 处理学习算法的方法

许多分类算法在同一训练数据集上多次执行的算法可能得到不同的模型。如对于神经网络的拓扑结构和各个神经元之间联系的初始权值的设定，对于决策树可以通过在树的生成过程中注入随机性因素（结点划分最好的k个最好属性中随机选择一个）。

# 基本分类器分类结果的整合

现在我们有多个基本分类器的分类结果了，但是怎么根据这么多的分类结果来给出最终决策呢？基本分类器的整合方式归纳为三个层次：


> 抽象层次：每个基本分类器只提供一个目标分类或者目标分类子集；

> 排位层次：每个基本分类器提供一个可能的目标分类列表，其中的目标分类按照可能性大小排列；

> 度量层次：每个基本分类不仅提供分类结果，还提供每种分类结果的可能性。

本文中我们主要介绍抽象层次的整合方式，我们把这种整合方式归为四类，并分别进行说明：

+ 简单投票法

它的基本思想是多个基本分类器都进行分类预测，然后根据分类结果用某种投票的原则进行投票表决，按照投票原则的不同投票法可以有一票否决、一致表决、少数服从多数、阈值表决等等。


> 一票否决：当且仅当所有分类器都把实例x划分到类Ci的时候才把x划分到类Ci，否则拒绝该实例；

> 一致表决：当且仅当没有分类器反对把x划分到类Ci的时候才把x划分到类Ci；

> 少数服从多数：让各个基本分类器进行投票（加权或者不加权），得票数多的那个分类Ci作为对应实例x的最终类别；

> 阈值表决：统计出把实例x划分为Ci和不划分为Ci的分类器数目，当这两者比例超过某个阈值的时候把x划分到类Ci；

此外，对于连续目标变量的分类问题，还可以采用平均取值、加权求和、中位数、最大值等等方式进行整合。

+ 贝叶斯投票法

简单投票法假设每个基本分类器都是平等的，没有分类能力之间的差别，但是这种假设并不总是合适的，在实际生活中，我们听取一个人的意见的时候会考虑到这个人过去的意见是否有用，贝叶斯投票法就是基于这种思想来提出的。贝叶斯投票法是基于每一个基本分类器在过去的分类表现来设定一个权值，然后按照这个权值进行投票，其中每个基本分类器的权值基于贝叶斯定理来进行计算。

虽然理论上贝叶斯投票法在假设空间所有假设的先验概率都正确的情况下能够获得最优的集成效果，但是实际应用中往往不可能穷举整个假设空间，也不可能准确地给每个假设分配先验概率，从而使得在实际使用中其他集成方法也会优于贝叶斯投票法。

# 集成学习方法对比

+ Bagging方法得到的集成分类器几乎总是比单个分类器效果要好；

+ Boosting方法得到的集成分类器的效果可能明显优于Bagging方法得到的分类器和单个分类器；

+ 然而对于某些数据集Boosting的结果有可能比单个分类器的性能还低；

+ 使用随机化人工神经网络的初始权值来进行集成的方法往往能够取得和Bagging同样好的效果；

+ Boosting方法的效果至少有一定程度依赖于数据集，而Bagging方法对数据集的依赖却没有那么明显。
