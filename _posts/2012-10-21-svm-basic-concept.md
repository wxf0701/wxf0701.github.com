--- 
layout: post
title: 支持向量机(SVM)：基本概念
tags: 
- "机器学习"
- "分类"
- "SVM"
status: publish
type: post
published: true
---
统计学习方法之间的不同时，首先要确定模型的假设空间，我们需要学习什么样的模型，是概率模型还是非概率模型，是线性模型还是非线性模型。如果决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间中的模型一般有无穷多个，如何确定选取具体哪一个模型？这时就需要考虑按照什么样的学习策略/准则进行学习，以及求解最优模型的具体计算方法/算法。其中，求解最优模型一般采用已有或定制的优化算法来保证高效求得全局最优解；而模型的学习策略是要确定模型质量的好坏，对于监督学习就是要确定模型一次预测的好坏和平均意义下模型预测的好坏。

# 损失函数和期望风险（Loss Function & Expected Risk）

我们用损失函数度量模型一次预测的好坏，用期望风险度量平均意义下模型预测的好坏。对于监督学习过程，我们是在模型的假设空间中选择模型f作为决策函数，它对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实的Y可能一致也可能不一致。我们用损失函数来度量模型预测错误的程度，将其记作L(Y,f(x))，为f(x)和Y的非负实值函数，常见的损失函数有如下几种：

（1）0-1损失函数（0-1 loss function）

![SVM](/upload/pic/2012-10-21-svm-basic-concept-1.png "")

（2）平方损失函数(quadratic loss function)

![SVM](/upload/pic/2012-10-21-svm-basic-concept-2.png "")

（3）绝对损失函数(absolute loss function)

![SVM](/upload/pic/2012-10-21-svm-basic-concept-3.png "")

（4）对数损失函数(logarithmic loss function)/对数似然损失函数(log-likelihood loss function)

![SVM](/upload/pic/2012-10-21-svm-basic-concept-4.png "")

损失函数的值越小，模型就越好。由于输入、输出（X，Y）是随机变量，遵循联合分布P（X,Y），所以损失函数的期望（或者说期望风险）为

![SVM](/upload/pic/2012-10-21-svm-basic-concept-5.png "")

这是理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失。学习的目标就是选择期望风险最小的模型，但是联合分布P(X,Y)是未知的，因此期望风险无法直接计算（实际上，如果知道联合分布P(X,Y)，就可以直接从联合分布直接求出条件概率分布P(Y|X)，也就不需要学习了）。真是的损失/误差无从得知，怎么办？我们可以用已经掌握的数据去近似逼近它，最直观的想法就是用模型f(X)在训练数据集上的平均损失来逼近，这就是所谓的经验风险（Empirical Risk）。

# 经验风险最小化（Empirical Risk Minimization）

经验风险是模型f(x)关于训练样本集的平均损失，我们将其记为Remp，其定义如下：

![SVM](/upload/pic/2012-10-21-svm-basic-concept-6.png "")

根据大数定律，当样本量N趋向于无穷大时，经验风险趋向于期望风险。但是，现实中训练样本的容量毕竟有限，甚至可能很小。所以直接用经验风险估计期望风险常常不理想，要对经验风险进行一定的矫正，这就涉及到监督学习的两个基本策略：经验风险最小化和结构风险最小化。

经验风险最小化的策略认为，经验风险最小的模型是最优模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：

![SVM](/upload/pic/2012-10-21-svm-basic-concept-7.png "")

上式中，F为模型的假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛应用。比如极大似然估计（Maximum likelihood estimation）就是经验风险最小的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

然而当训练样本容量很小时，经验最小化原则往往只是在占很小比例的训练样本上做到没有误差或误差很小，模型在应用中不能保证在更大比例的真实样本中表现同样好，往往表现非常之差，也就是说模型的推广能力或泛化能力无法得到保证，产生了过拟合现象。

# VC维（Vapnik-Chervonenkis Dimension）

由经验风险最小化的原则可知，选出经验风险最小的映射f是问题的关键，那么追根溯源找到合适的函数集F。从某种意义上来说，F的丰富程度，或者说“表达能力”是选择最佳f的基础。由此，“VC维”的理念应运而生，它是由Vapnik和Chervonenkis提出的，是对F“表达能力”的一种描述。

VC维的直观定义是：对一个指示函数集，如果存在h个样本能够被函数集中的函数按所有可能的2h种形式分开，则称函数集能够把h个样本打散。函数集的VC维就是它能打散的最大样本数目h。若对任意数目的样本都有函数能将它们打散，则函数集的VC维是无穷大。VC维反映了函数集的学习能力，VC维越大则学习机器越复杂（容量越大），遗憾的是，目前尚没有通用的关于任意函数集VC维计算的理论，只对一些特殊的函数集知道其VC维。例如在N维空间中线形分类器和线形实函数的VC维是n+1，因此，平面内只能找到3个点能被直线打散而不找到第4个。

下面举个例子来直观说明VC维，设X为二维空间X={[x]1,[x]2}，F是X上的线性指示函数集合，即：F={f(x,a)=sgn(a2[x]2+a1[x]1+a0)}，令z3={x1,x2,x3}∈X，且x1,x2,x3不共线，对x1,x2,x3分别标上“X”标号和“O”标号，则共有23=8种标号方式，对每一种标号方式，若总是存在f∈F，使得不同的标号被分开，如下图所示，则说明该函数集的VC维为3。

![SVM](/upload/pic/2012-10-21-svm-basic-concept-8.png "")

对于4个点的情况，如下图所示，当类标号如下分布时，线性函数集合无法将其打散。

![SVM](/upload/pic/2012-10-21-svm-basic-concept-9.png "")

# 结构风险最小化（Structural Risk Minimization）

经验风险最小化原则容易产生过拟合主要是因为产生的模型过于复杂，结构风险最小化是为了防止过拟合而提出来的策略，它在经验风险风险最小的基础上加上了表示模型复杂度度的惩罚项（Penalty Term）。在模型假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义如下：

![SVM](/upload/pic/2012-10-21-svm-basic-concept-10.png "")

上式中，J(F)为模型的复杂度，模型越复杂，复杂度J(F)就越大，反之，则越小。也就是说，模型复杂度表示了对模型的惩罚。λ为系数，用来权衡经验风险和模型复杂度。结构风险小要求经验风险和模型复杂度都要小，因此结构风险小的模型对训练样本和未知的测试数据都能表现出较好的性能。

与经验风险最小化类似，结构风险最小化策略认为结构风险最小的模型为最优模型，所以求最优模型，就是求解最优化问题：

![SVM](/upload/pic/2012-10-21-svm-basic-concept-11.png "")

贝叶斯估计中的最大后验概率估计（Maximum posterior probability estimation, MAP）就是结构风险最小化的例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。
