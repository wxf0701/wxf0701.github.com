--- 
layout: post
title: 决策树的过分拟合问题及解决方法
tags: 
- "机器学习"
- "分类"
- "决策树"
status: publish
type: post
published: true
---
一个好的分类模型不仅要能够很好的拟合训练数据，而且还应该对未知样本能准确分类。也就是说，一个好的分类模型必须同时具有低训练误差和低泛化误差。对训练数据拟合过好的模型，其泛化误差可能比具有较高训练误差的模型高，这种情况就是所谓模型过分拟合。本文介绍可能导致决策树过份拟合的原因及其解决方法。

过分拟合的原因

1〉噪声数据导致过分拟合

在现实世界中，数据伴有随机的错误或噪声往往是难以完全避免的。例如在对用户是否离网的分类中，目标变量”是否流失“可能被错误的标记，利用此数据拟合得到的模型，就有可能因为拟合误标记的训练记录，导致在模型应用阶段产生错误分类，不能很好的进行推广；

2〉缺乏代表性样本导致过分拟合

在训练数据缺乏具有代表性的样本的情况下，往往需要继续细化模型才能得到较好拟合训练集的模型，这样得到的模型同样可能具有较高的泛化误差；

避免过分拟合的策略

与上述两个导致模型过分拟合的因素同时出现的是模型的复杂度，模型越复杂，出现过分拟合的概率就越高，因此，对于给定具有相同泛化误差的模型，我们往往更倾向于较为简单的模型，这就是所谓的Occam剃刀（Occam’s razor）原则。为了避免过分拟合，降低决策树的复杂度，通常的策略是剪枝，该策略采用统计方法删去最不可靠的分支（树枝），以提高未来分类识别的速度和分类识别新数据的能力。

1〉事前修剪（pre-pruning）法 

事先剪枝法通过提前停止分支的生长过程，即通过在当前结点上就判断是否需要继续划分该结点所含训练样本集来实现； 为了做到这一点，就需要更为限制性的约束条件，如当观察到不纯性度量的增益低于某个确定的阀值时就停止扩展分支的生长。

该方法的优点在于避免产生过分拟合训练数据的过于复杂的子树，但是，我们很难为提前终止选取正确的阀值，阀值太高将导致拟合不足的模型，而阀值太低则不能充分地解决过分拟合问题。此外，即便是使用已有的属性测试条件得不到显著的增益，接下来的划分也可能产生较好的子树。

2〉事后剪枝（post-pruning）法

事后剪枝法从一个“充分生长”树中，按照自底向上的方式修剪掉多余的分支，修剪有两种方法：（1）用新的叶子结点替换子树，该叶子结点的类标号由子树记录中的多数类确定；（2）用子树中最常用的分支代替子树。J48决策树算法采用了子树提升（subtree raising）与子树替换（subtree replacement）的修剪策略。

计算修剪前后的预期分类错误率，如果修剪导致预期分类错误率变大，则放弃修剪，保留相应结点的各个分支，否则就将相应结点分支修剪删去。在产生一系列经过修剪的决策树候选之后，利用一个独立的测试数据集，对这些经过修剪的决策树的分类准确性进行评价， 保留下预期分类错误率最小的 （修剪后） 决策树。

与事先剪枝相比，事后剪枝倾向于产生更好的结果，因为与事先剪枝不同，事后剪枝是根据完全生长的树做出的剪枝决策，先剪枝则可能过早终止决策树的生长。
