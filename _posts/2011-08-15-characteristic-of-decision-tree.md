--- 
layout: post
title: 决策树归纳算法的特点
tags: 
- "机器学习"
- "决策树"
status: publish
type: post
published: true
---
1〉决策树归纳是一种构建分类模型的非参数方法。换言之，它不要求任何先验假设，不假定类和其他属性服从一定的概率分布（如Logistic回归）；

2〉找到最优决策树是NP完全问题，许多决策树算法都采取启发式方法指导对假设空间的搜索，如采用贪心的、自顶向下的递归划分策略建立决策树；

3〉不需要昂贵的计算代价，即使训练集非常大，也可以快速建立模型。此外，决策树一旦建立，未知样本分类也非常快，最坏情况下的时间复杂度为O(w)，其中w是树的最大深度；

4〉决策树相对容易解释，特别是小型决策树；在很多简单的数据集上，决策树的准确率也可以与其他分类算法想媲美；此外，算法还会将最为重要的判断因素安排在靠近树根部的位置，这对决策过程的解释也非常有帮助；

5〉决策树算法对于噪声的干扰具有相当好的鲁棒性，采用避免过分拟合的方法之后尤其如此；

6〉冗余属性不会对决策树的准确率造成不利影响。如果一个属性在数据集中与另一个属性是强相关的，则该属性是冗余的。在两个冗余属性中，如果已经选择其中一个作为用于划分的属性，则另一个属性将被忽略；另一方面，如果数据集中含有很多与目标变量不相关的属性，则某些属性可能在树的构造过程中偶然被选中，导致决策树过于庞大。在预处理阶段，特征选择技术能够帮助找出并删除不相关属性，以帮助提高决策树的准确率； 

7〉数据碎片（data fragmentation）问题。由于大多数的决策树算法采用自顶向下的递归划分方法。因此沿着树向下，记录会越来越少，在叶子结点记录数量可能太少，以致对于叶子结点代表的类的判决不具有统计显著性。解决这一问题的方法是当样本数小于某个特定阀值时停止分裂；

8〉由于大多数决策树算法采用分治划分策略，因此属性空间的不同部分可能使用相同的测试条件，从而导致子树在决策书中重复出现多次，这使得决策树过于复杂，并且可能更难于解释；

9〉决策树是学习离散值函数的典型代表，然而，它不能很好的推广到某些特定的布尔问题。如对于奇偶函数来说，当奇数/偶数个布尔属性为真时其值为0/1，对这样的函数准确建模需要一棵具有2的d次方个节点的满决策树（d为布尔属性的个数）。

10〉对于测试条件只涉及一个属性的决策树，可以将决策树的成长过程看成划分属性空间为不相交的区域的过程，直到每个区域都只包含同一记录，两个不同类的相邻区域之间的边界称作决策边界（Decision Boundary）。由于测试属性只涉及单个属性，因此决策边界是平行于坐标轴的直线，这就限制了决策树对连续属性之间复杂关系建模的表达能力。斜决策树（Oblique Decision Tree）是克服以上局限的方法之一，它允许测试条件涉及多个属性（如x+y<1），当然，这一技术在赋予决策树更强表达能力的同时，为给定结点找出最佳的测试条件的计算复杂度也是昂贵的。另一种将属性空间划分为非矩形区域的方法是构造归纳（Constructive Induction），即创建复合属性用于代表已有属性的算术/逻辑组合，该方法不需要昂贵的计算花费；

11〉研究表明大部分不纯性度量方法的结果是指一致的，因此不纯性度量方法的选择对决策树算法的性能影响很小。树剪枝对最终决策树的影响往往比不纯性度量的选择影响更大；

12〉决策树不支持增量式训练，每当新的开用训练样本到来时，只能从头开始训练。在某些应用场景下，每次从头开始训练是不切实际的。
