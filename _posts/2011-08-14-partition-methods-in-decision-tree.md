--- 
layout: post
title: 决策树选择最佳划分的方法
tags: 
- "机器学习"
- "决策树"
status: publish
type: post
published: true
---
我们已经讨论过决策树归纳设计的两个问题：1〉如何分裂训练记录；2〉如何停止分裂生长；对于第一个问题，决策树算法首先为不同类型的属性（二元属性、标称属性、序数属性、连续属性）提供表示属性测试条件和对应输出的方法。如对于连续属性，若要生成二元划分，决策树算法需要考虑所有的划分点，并从中选择产生最佳划分的点，若要生成多元划分，则必须考虑所有可能的连续区间，这种情况下，可以先对属性值进行离散化处理，然后对每一个离散区间赋予一个新的序数值，只要保证有序性，相邻的值还可以聚集成较宽的区间。

然后，更重要的是要评估这些测试条件生成的划分是否是局部最优的，大多数度量方法用给定测试条件划分前和划分后记录的类分布来定义。

# 不纯性的度量

设P(i|t)表示给定结点t中属于类i的记录所占的比例，则在二元分类问题中，任意结点的类分布比例可以记为P(i=0|t)和P(i=1|t)，且P(i=0|t) + P(i=1|t) = 1；选择最佳划分的度量通常是根据划分后子女节点的不纯性的程度，不纯的程度越低，类分布就越倾斜。设c为类的个数，以下是几个不纯性度量的例子：


> Gini(t)= 1 – P(0|t)*P(0|t) – P(1|t)*P(1|t) – P(2|t)*P(2|t) – P(c-1|t)*P(c-1|t)

> Entropy(t) = – P(0|t)*logP(0|t) – P(1|t)*logP(1|t) – P(2|t)*logP(2|t) – … – P(c-1|t)*logP(c-1|t)

> Error(t) = 1 – max{P(0|t) , P(1|t) , P(2|t) , … , P(c-1|t)}

根据以上公式，假设有含有类别0和类别1的三个结点N1、N2和N3，每个结点中类别0和类表1的计数分别为(0,6)、(1,5)、(3,3)，则各个结点不纯性度量分别为：

对于结点N1：


> Gini = 1 – 0/6 * 0/6 – 6/6 * 6/6 = 0

> Entropy = –0/6*log(0/6) – 6/6*log(6/6) = 0

> Error = 1 – max{0/6 , 6/6} = 0

对于结点N2：


> Gini = 1 – 1/6 * 1/6 – 5/6 * 5/6 = 0.278

> Entropy = –1/6*log(1/6) – 5/6*log(5/6) = 0.650

> Error = 1 – max{1/6 , 6/6} = 0.167

对于结点N3：


> Gini = 1 – 3/6 * 3/6 – 3/6 * 3/6 = 0.5

> Entropy = –3/6*log(3/6) – 3/6*log(3/6) = 1

> Error = 1 – max{3/6 , 3/6} = 0.5

从以上可以看出，三种方法的不纯性度量是一致的：都在类分布平衡时达到最大值，而当所有记录属于同一类别时达到最小值；结点N1 具有最低的不纯性度量值，其次是N2，结点N3具有最高的不纯性度量值。

测试条件的评估：增益Δ

为了确定测试条件的效果，为对各个测试条件的选择提供依据，我们需要比较划分前（父结点）的不纯程度和划分后（子女结点）的不纯程度，它们的差值越大，说明测试条件的效果就越好。增益Δ就是一种可以用来确定划分效果的标准：


> Δ=I(parent) – [ N(V1)/N * Ι(V1) + N(V2)/N * I(V2) + … +  N(Vk)/N * I(Vk) ]

其中，I(.)是给定结点的不纯性度量，N是父节点的记录个数，k是划分的个数，N(Vk)是与子女结点Vk的记录个数。决策树算法通常选择最大化增益等价于最小化子女结点的不纯性度量的加权平均值。当选择熵(Entropy)作为增益Δ的不纯性度量时，熵的差就是所谓信息增益(Information Gain)。

# 测试条件的评估：增益率

熵和Gini指标等不纯性度量趋向有利于具有大量不同值的属性。例如考察预测用户的是否流失，对于两个测试条件“客户级别”和“客户ID”，后者必然产生更纯的子女结点，这可以说明“客户ID”提供了更好的划分记录的方法吗？显然不是，因为每个样本在属性“客户ID”上的值都是唯一的，它本身就不是一个有预测性的属性。诚然，这是一种极端情况，但是即便是不太极端的情况下，产生大量输出的测试条件仍然不是我们所希望的，因为与每个划分相关联的记录太少，以至不能作出可靠的预测。

解决该问题的策略有两种。第一种策略是限制测试条件只能是二元划分，CART决策树算法就是采用了该策略；另一种策略修改评估划分的标准，把属性测试条件产生的输出数也考虑进去，例如C4.5决策树算法采用增益率(Gain Ratio)作为划分标准来评估划分，增益率定义如下：


> Gain Ratio = Information Gain/Split Info

> Split Info = – P(V1) * logP(V1) – P(V2) * logP(V2) – … – P(Vk) * logP(Vk) 

其中，k是划分的数量。例如，如果每个属性值具有相同的记录数，则对于任意的划分i有P(Vi)=1/k，而划分信息Split Info = log(k)。这说明如果一个属性产生了大量的划分，它的划分信息就会很大，从而降低了增益率Gain Ratio。
