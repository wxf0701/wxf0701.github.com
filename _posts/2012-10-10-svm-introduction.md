--- 
layout: post
title: 支持向量机(SVM)：简介
tags: 
- "机器学习"
- "分类"
- "SVM"
status: publish
type: post
published: true
---
支持向量机（Support Vector Machine, SVM）是Cortes和Vapnik于1995年首先提出，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。它建立在统计学习理论的VC（Vapnik-Chervonenkis）维理论和结构风险最小（SRM）原理基础之上，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力。

支持向量机SVM的基本思想

支持向量机最早应用于二类分类问题，其主要目标是寻找线性可分情况下的最优分类超平面。它的基本思想比较直观，下图中加号、减号两类圆点分别代表两类样本，选定方向ω，取一个分类超平面H，平行移动H得到的两个极端位置H1和H2，都可以将两类样本正确区分开来，取H= (H1+H2)/2为最优分类超平面。很明显，最优分类超平面就是使得加号、减号两类圆点代表的两类样本不但能够被正确划分开来，而且分类间隔要达到最大。

basic

支持向量机SVM的分类

（1） 线性可分支持向量机（Linear Support Vector Machine in Linearly Separable Case）

当数据线性可分时，通过硬间隔最大化（Hard Margin Maximization），学习一个线性分类器，即线性可分支持向量机，也称为硬间隔支持向量机。

（2） 线性支持向量机（Linear Support Vector Machine）

当训练数据近似线性可分时，通过软间隔最大化（Soft Margin Maximization），学习一个线性分类器，即线性支持向量机，也成为软间隔支持向量机。

（3） 非线性支持向量机（Non- Linear Support Vector Machine）

当训练数据线性不可分时，通过使用核技巧（Kernel Trick）及软间隔最大化，学习非线性支持向量机。

支持向量机SVM的特点

（1）非线性映射是SVM方法的理论基础，SVM向高维空间的非线性映射采用的是内积核函数代替的方法；

（2）最大化分类边际的思想是SVM的核心，对特征空间划分的最优超平面是SVM的目标；

（3）支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。

（4）SVM 是一种有坚实理论基础的新颖的小样本学习方法。它基本上不涉及概率测度及大数定律等。从本质上看，它避开了从归纳到演绎的传统过程，实现了高效的从训练样本到预报样本的“转导推理”，大大简化了通常的分类和回归等问题。

（5）SVM 的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。

（6）少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本，而且注定了该方法不但算法简单，而且具有较好的“鲁棒”性。这种“鲁棒”性主要体现在：①增、删非支持向量样本对模型没有影响；②支持向量样本集具有一定的鲁棒性；③有些成功的应用中，SVM 方法对核的选取不敏感。
